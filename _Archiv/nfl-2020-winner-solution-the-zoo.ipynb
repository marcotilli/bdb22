{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "from kaggle.competitions import nflrush\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "682154"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = nflrush.make_env()\n",
    "train = pd.read_csv('../input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})\n",
    "\n",
    "# Original dataframe:\n",
    "#train = pd.read_csv('train.csv', dtype={'WindSpeed': 'object'})\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First split into player-specific data and play-specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_play_and_player_cols(df,predicting=False):\n",
    "    df['IsRusher'] = df['NflId'] == df['NflIdRusher']\n",
    "    \n",
    "    play_ids = df[\"PlayId\"].unique()\n",
    "    #play_ids_filter = np.random.choice(play_ids,int(len(play_ids)*0.01),replace=False)\n",
    "    #df = df.loc[df.PlayId.isin(play_ids_filter)]\n",
    "    \n",
    "    df['PlayId'] = df['PlayId'].astype(str)\n",
    "    \n",
    "    # We must assume here that the first 22 rows correspond to the same player:\n",
    "    player_cols = [\n",
    "        'PlayId', # This is the link between them\n",
    "        'Season',\n",
    "        'Team',\n",
    "        'X',\n",
    "        'Y',\n",
    "        'S',\n",
    "        'A',\n",
    "        'Dis',\n",
    "        'Dir',\n",
    "        'NflId',\n",
    "        'IsRusher',\n",
    "    ]\n",
    "\n",
    "    df_players = df[player_cols]\n",
    "    \n",
    "    play_cols = [\n",
    "        'PlayId',\n",
    "        'Season',\n",
    "        'PossessionTeam',\n",
    "        'HomeTeamAbbr',\n",
    "        'VisitorTeamAbbr',\n",
    "        'PlayDirection', \n",
    "        'FieldPosition',\n",
    "        'YardLine',\n",
    "    ]\n",
    "    if not predicting:\n",
    "        play_cols.append('Yards')\n",
    "        \n",
    "    df_play = df[play_cols].copy()\n",
    "\n",
    "    ## Fillna in FieldPosition attribute\n",
    "    #df['FieldPosition'] = df.groupby(['PlayId'], sort=False)['FieldPosition'].apply(lambda x: x.ffill().bfill())\n",
    "    \n",
    "    # Get first \n",
    "    df_play = df_play.groupby('PlayId').first().reset_index()\n",
    "\n",
    "    #print('rows/plays in df: ', len(df_play))\n",
    "    assert df_play.PlayId.nunique() == df.PlayId.nunique(), \"Play/player split failed?\"  # Boom\n",
    "    \n",
    "    return df_play, df_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_ids = train[\"PlayId\"].unique()\n",
    "\n",
    "df_play, df_players = split_play_and_player_cols(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team Abbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_team_abbr(df):\n",
    "\n",
    "    #These are only problems:\n",
    "    map_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\n",
    "    for abb in df['PossessionTeam'].unique():\n",
    "        map_abbr[abb] = abb\n",
    "\n",
    "    df['PossessionTeam'] = df['PossessionTeam'].map(map_abbr)\n",
    "    df['HomeTeamAbbr'] = df['HomeTeamAbbr'].map(map_abbr)\n",
    "    df['VisitorTeamAbbr'] = df['VisitorTeamAbbr'].map(map_abbr)\n",
    "\n",
    "    df['HomePossession'] = df['PossessionTeam'] == df['HomeTeamAbbr']\n",
    "    \n",
    "    return\n",
    "\n",
    "process_team_abbr(df_play)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PlayDirection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_play_direction(df):\n",
    "    df['IsPlayLeftToRight'] = df['PlayDirection'].apply(lambda val: True if val.strip() == 'right' else False)\n",
    "    return\n",
    "process_play_direction(df_play)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yards Til End Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_yard_til_end_zone(df):\n",
    "    def convert_to_yardline100(row):\n",
    "        return (100 - row['YardLine']) if (row['PossessionTeam'] == row['FieldPosition']) else row['YardLine']\n",
    "    df['Yardline100'] = df.apply(convert_to_yardline100, axis=1)\n",
    "    return\n",
    "\n",
    "process_yard_til_end_zone(df_play)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tracking Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players = df_players.merge(\n",
    "    df_play[['PlayId', 'PossessionTeam', 'HomeTeamAbbr', 'PlayDirection', 'Yardline100']], \n",
    "    how='left', on='PlayId')\n",
    "\n",
    "df_players.loc[df_players.Season == 2017, 'S'] = 10*df_players.loc[df_players.Season == 2017,'Dis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_direction(df):\n",
    "    # adjusted the data to always be from left to right\n",
    "    df['HomePossesion'] = df['PossessionTeam'] == df['HomeTeamAbbr']\n",
    "\n",
    "    df['Dir_rad'] = np.mod(90 - df.Dir, 360) * math.pi/180.0\n",
    "\n",
    "    df['ToLeft'] = df.PlayDirection == \"left\"\n",
    "    df['TeamOnOffense'] = \"home\"\n",
    "    df.loc[df.PossessionTeam != df.HomeTeamAbbr, 'TeamOnOffense'] = \"away\"\n",
    "    df['IsOnOffense'] = df.Team == df.TeamOnOffense # Is player on offense?\n",
    "    df['X_std'] = df.X\n",
    "    df.loc[df.ToLeft, 'X_std'] = 120 - df.loc[df.ToLeft, 'X']\n",
    "    df['Y_std'] = df.Y\n",
    "    df.loc[df.ToLeft, 'Y_std'] = 160/3 - df.loc[df.ToLeft, 'Y']\n",
    "    df['Dir_std'] = df.Dir_rad\n",
    "    df.loc[df.ToLeft, 'Dir_std'] = np.mod(np.pi + df.loc[df.ToLeft, 'Dir_rad'], 2*np.pi)\n",
    "   \n",
    "    #Replace Null in Dir_rad\n",
    "    df.loc[(df.IsOnOffense) & df['Dir_std'].isna(),'Dir_std'] = 0.0\n",
    "    df.loc[~(df.IsOnOffense) & df['Dir_std'].isna(),'Dir_std'] = np.pi\n",
    "\n",
    "standarize_direction(df_players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(df, sample_ids):\n",
    "    df_sample = df.loc[df.PlayId.isin(sample_ids)].copy()\n",
    "    df_sample['Y_std'] = 160/3  - df_sample['Y_std']\n",
    "    df_sample['Dir_std'] = df_sample['Dir_std'].apply(lambda x: 2*np.pi - x)\n",
    "    df_sample['PlayId'] = df_sample['PlayId'].apply(lambda x: x+'_aug')\n",
    "    return df_sample\n",
    "\n",
    "def process_tracking_data(df):\n",
    "    # More feature engineering for all:\n",
    "    df['Sx'] = df['S']*df['Dir_std'].apply(math.cos)\n",
    "    df['Sy'] = df['S']*df['Dir_std'].apply(math.sin)\n",
    "    \n",
    "    # ball carrier position\n",
    "    rushers = df[df['IsRusher']].copy()\n",
    "    rushers.set_index('PlayId', inplace=True, drop=True)\n",
    "    playId_rusher_map = rushers[['X_std', 'Y_std', 'Sx', 'Sy']].to_dict(orient='index')\n",
    "    rusher_x = df['PlayId'].apply(lambda val: playId_rusher_map[val]['X_std'])\n",
    "    rusher_y = df['PlayId'].apply(lambda val: playId_rusher_map[val]['Y_std'])\n",
    "    rusher_Sx = df['PlayId'].apply(lambda val: playId_rusher_map[val]['Sx'])\n",
    "    rusher_Sy = df['PlayId'].apply(lambda val: playId_rusher_map[val]['Sy'])\n",
    "    \n",
    "    # Calculate differences between the rusher and the players:\n",
    "    df['player_minus_rusher_x'] = rusher_x - df['X_std']\n",
    "    df['player_minus_rusher_y'] = rusher_y - df['Y_std']\n",
    "\n",
    "    # Velocity parallel to direction of rusher:\n",
    "    df['player_minus_rusher_Sx'] = rusher_Sx - df['Sx']\n",
    "    df['player_minus_rusher_Sy'] = rusher_Sy - df['Sy']\n",
    "\n",
    "    return\n",
    "\n",
    "sample_ids = np.random.choice(df_play.PlayId.unique(), int(0.5*len(df_play.PlayId.unique())))\n",
    "#sample_ids = df_play.PlayId.unique()\n",
    "\n",
    "df_players_aug = data_augmentation(df_players, sample_ids)\n",
    "df_players = pd.concat([df_players, df_players_aug])\n",
    "df_players.reset_index()\n",
    "\n",
    "df_play_aug = df_play.loc[df_play.PlayId.isin(sample_ids)].copy()\n",
    "df_play_aug['PlayId'] = df_play_aug['PlayId'].apply(lambda x: x+'_aug')\n",
    "df_play = pd.concat([df_play, df_play_aug])\n",
    "df_play.reset_index()\n",
    "\n",
    "# This is necessary to maintain the order when in the next cell we use groupby\n",
    "df_players.sort_values(by=['PlayId'],inplace=True)\n",
    "df_play.sort_values(by=['PlayId'],inplace=True)\n",
    "\n",
    "process_tracking_data(df_players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any null values:  0\n"
     ]
    }
   ],
   "source": [
    "tracking_level_features = [\n",
    "    'PlayId',\n",
    "    'IsOnOffense',\n",
    "    'X_std',\n",
    "    'Y_std',\n",
    "    'Sx',\n",
    "    'Sy',\n",
    "    'player_minus_rusher_x',\n",
    "    'player_minus_rusher_y',\n",
    "    'player_minus_rusher_Sx',\n",
    "    'player_minus_rusher_Sy',\n",
    "    'IsRusher'\n",
    "]\n",
    "\n",
    "df_all_feats = df_players[tracking_level_features]\n",
    "\n",
    "print('Any null values: ', df_all_feats.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35min 28s, sys: 3.71 s, total: 35min 32s\n",
      "Wall time: 35min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "grouped = df_all_feats.groupby('PlayId')\n",
    "train_x = np.zeros([len(grouped.size()),11,10,10])\n",
    "i = 0\n",
    "play_ids = df_play.PlayId.values\n",
    "for name, group in grouped:\n",
    "    if name!=play_ids[i]:\n",
    "        print(\"Error\")\n",
    "\n",
    "    [[rusher_x, rusher_y, rusher_Sx, rusher_Sy]] = group.loc[group.IsRusher==1,['X_std', 'Y_std','Sx','Sy']].values\n",
    "\n",
    "    offense_ids = group[group.IsOnOffense & ~group.IsRusher].index\n",
    "    defense_ids = group[~group.IsOnOffense].index\n",
    "\n",
    "    for j, defense_id in enumerate(defense_ids):\n",
    "        [def_x, def_y, def_Sx, def_Sy] = group.loc[defense_id,['X_std', 'Y_std','Sx','Sy']].values\n",
    "        [def_rusher_x, def_rusher_y] = group.loc[defense_id,['player_minus_rusher_x', 'player_minus_rusher_y']].values\n",
    "        [def_rusher_Sx, def_rusher_Sy] =  group.loc[defense_id,['player_minus_rusher_Sx', 'player_minus_rusher_Sy']].values\n",
    "        \n",
    "        train_x[i,j,:,:4] = group.loc[offense_ids,['Sx','Sy','X_std', 'Y_std']].values - np.array([def_Sx, def_Sy, def_x,def_y])\n",
    "        train_x[i,j,:,-6:] = [def_rusher_Sx, def_rusher_Sy, def_rusher_x, def_rusher_y, def_Sx, def_Sy]\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform train_y to one hot encoded vector\n",
    "\n",
    "Then we train it with logloss function and directly predict pdf (then transform to cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max yardIndex:  198\n",
      "max yardIndexClipped:  150\n",
      "min yardIndex:  84\n",
      "min yardIndexClipped:  84\n"
     ]
    }
   ],
   "source": [
    "# Transform Y into indexed-classes:\n",
    "train_y = df_play[['PlayId', 'Yards']].copy()\n",
    "\n",
    "train_y['YardIndex'] = train_y['Yards'].apply(lambda val: val + 99)\n",
    "\n",
    "min_idx_y = 71\n",
    "max_idx_y = 150\n",
    "\n",
    "train_y['YardIndexClipped'] = train_y['YardIndex'].apply(\n",
    "    lambda val: min_idx_y if val < min_idx_y else max_idx_y if val > max_idx_y else val)\n",
    "\n",
    "print('max yardIndex: ', train_y.YardIndex.max())\n",
    "print('max yardIndexClipped: ', train_y.YardIndexClipped.max())\n",
    "print('min yardIndex: ', train_y.YardIndex.min())\n",
    "print('min yardIndexClipped: ', train_y.YardIndexClipped.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season = df_play[['PlayId', 'Season']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ConvNet\n",
    "\n",
    "\n",
    "Below class Metric based entirely on: https://www.kaggle.com/kingychiu/keras-nn-starter-crps-early-stopping\n",
    "\n",
    "Below early stopping entirely based on: https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/112868#latest-656533\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, AvgPool1D, AvgPool2D, Reshape,\n",
    "    Input, Activation, BatchNormalization, Dense, Add, Lambda, Dropout, LayerNormalization)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "\n",
    "import tensorflow as tf \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def crps(y_true, y_pred):\n",
    "    loss = K.mean(K.sum((K.cumsum(y_pred, axis = 1) - K.cumsum(y_true, axis=1))**2, axis=1))/199\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_idx_y = 71\n",
    "max_idx_y = 150\n",
    "num_classes_y = max_idx_y - min_idx_y + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_net(num_classes_y):\n",
    "    #_, x, y, z = train_x.shape\n",
    "    inputdense_players = Input(shape=(11,10,10), name = \"playersfeatures_input\")\n",
    "    \n",
    "    X = Conv2D(128, kernel_size=(1,1), strides=(1,1), activation='relu')(inputdense_players)\n",
    "    X = Conv2D(160, kernel_size=(1,1), strides=(1,1), activation='relu')(X)\n",
    "    X = Conv2D(128, kernel_size=(1,1), strides=(1,1), activation='relu')(X)\n",
    "    \n",
    "    # The second block of convolutions learns the necessary information per defense player before the aggregation.\n",
    "    # For this reason the pool_size should be (1, 10). If you want to learn per off player the pool_size must be \n",
    "    # (11, 1)\n",
    "    Xmax = MaxPooling2D(pool_size=(1,10))(X)\n",
    "    Xmax = Lambda(lambda x1 : x1*0.3)(Xmax)\n",
    "\n",
    "    Xavg = AvgPool2D(pool_size=(1,10))(X)\n",
    "    Xavg = Lambda(lambda x1 : x1*0.7)(Xavg)\n",
    "\n",
    "    X = Add()([Xmax, Xavg])\n",
    "    X = Lambda(lambda y : K.squeeze(y,2))(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    X = Conv1D(160, kernel_size=1, strides=1, activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Conv1D(96, kernel_size=1, strides=1, activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Conv1D(96, kernel_size=1, strides=1, activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    Xmax = MaxPooling1D(pool_size=11)(X)\n",
    "    Xmax = Lambda(lambda x1 : x1*0.3)(Xmax)\n",
    "\n",
    "    Xavg = AvgPool1D(pool_size=11)(X)\n",
    "    Xavg = Lambda(lambda x1 : x1*0.7)(Xavg)\n",
    "\n",
    "    X = Add()([Xmax, Xavg])\n",
    "    X = Lambda(lambda y : K.squeeze(y,1))(X)\n",
    "    \n",
    "    X = Dense(96, activation=\"relu\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    X = Dense(256, activation=\"relu\")(X)\n",
    "    X = LayerNormalization()(X)\n",
    "    X = Dropout(0.3)(X)\n",
    "\n",
    "    outsoft = Dense(num_classes_y, activation='softmax', name = \"output\")(X)\n",
    "\n",
    "    model = Model(inputs = [inputdense_players], outputs = outsoft)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(Callback):\n",
    "    def __init__(self, model, callbacks, data):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.callbacks = callbacks\n",
    "        self.data = data\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_train_begin(logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_train_end(logs)\n",
    "\n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        X_valid, y_valid = self.data[0], self.data[1]\n",
    "\n",
    "        y_pred = self.model.predict(X_valid)\n",
    "        y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n",
    "        y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n",
    "        val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * X_valid.shape[0])\n",
    "        logs['val_CRPS'] = val_s\n",
    "        \n",
    "        for callback in self.callbacks:\n",
    "            callback.on_epoch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Val loss: 0.012052951885909664\n",
      "Fold : 1\n",
      "Val loss: 0.012141012499985818\n",
      "Fold : 2\n",
      "Val loss: 0.011978110386260083\n",
      "Fold : 3\n",
      "Val loss: 0.011963890412402312\n",
      "Fold : 4\n",
      "Val loss: 0.012211667330337841\n",
      "Fold : 5\n",
      "Val loss: 0.01188167311846686\n",
      "0.01203821760556043\n",
      "CPU times: user 4h 1min 26s, sys: 9min 37s, total: 4h 11min 3s\n",
      "Wall time: 1h 19min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "models = []\n",
    "kf = KFold(n_splits=6, shuffle=True)\n",
    "score = []\n",
    "\n",
    "for i, (tdx, vdx) in enumerate(kf.split(train_x, train_y)):\n",
    "    print(f'Fold : {i}')\n",
    "    X_train, X_val = train_x[tdx], train_x[vdx],\n",
    "    y_train, y_val = train_y.iloc[tdx]['YardIndexClipped'].values, train_y.iloc[vdx]['YardIndexClipped'].values\n",
    "    season_val = df_season.iloc[vdx]['Season'].values\n",
    "\n",
    "    y_train_values = np.zeros((len(y_train), num_classes_y), np.int32)\n",
    "    for irow, row in enumerate(y_train):\n",
    "        y_train_values[(irow, row - min_idx_y)] = 1\n",
    "        \n",
    "    y_val_values = np.zeros((len(y_val), num_classes_y), np.int32)\n",
    "    for irow, row in enumerate(y_val - min_idx_y):\n",
    "        y_val_values[(irow, row)] = 1\n",
    "\n",
    "    val_idx = np.where(season_val!=2017)\n",
    "    \n",
    "    X_val = X_val[val_idx]\n",
    "    y_val_values = y_val_values[val_idx]\n",
    "\n",
    "    y_train_values = y_train_values.astype('float32')\n",
    "    y_val_values = y_val_values.astype('float32')\n",
    "    \n",
    "    model = get_conv_net(num_classes_y)\n",
    "\n",
    "    es = EarlyStopping(monitor='val_CRPS',\n",
    "                        mode='min',\n",
    "                        restore_best_weights=True,\n",
    "                        verbose=0,\n",
    "                        patience=10)\n",
    "    \n",
    "    es.set_model(model)\n",
    "    metric = Metric(model, [es], [X_val, y_val_values])\n",
    "\n",
    "    lr_i = 1e-3\n",
    "    lr_f = 5e-4\n",
    "    n_epochs = 30 \n",
    "\n",
    "    decay = (1-lr_f/lr_i)/((lr_f/lr_i)* n_epochs - 1)  #Time-based decay formula\n",
    "    alpha = (lr_i*(1+decay))\n",
    "    \n",
    "    opt = Adam(learning_rate=1e-3)\n",
    "    model.compile(loss=crps,\n",
    "                  optimizer=opt)\n",
    "    \n",
    "    model.fit(X_train,\n",
    "              y_train_values, \n",
    "              epochs=n_epochs,\n",
    "              batch_size=64,\n",
    "              verbose=0,\n",
    "              callbacks=[metric],\n",
    "              validation_data=(X_val, y_val_values))\n",
    "\n",
    "    val_crps_score = min(model.history.history['val_CRPS'])\n",
    "    print(\"Val loss: {}\".format(val_crps_score))\n",
    "    \n",
    "    score.append(val_crps_score)\n",
    "\n",
    "    models.append(model)\n",
    "    \n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01203821760556043\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cdf_prediction_model(predict_x, n_classes=None, model=None, min_idx=None, max_idx=None, yardline100=None):\n",
    "    '''\n",
    "    predict_x - array-like of shape [nsamples, n_features]\n",
    "    min_idx - minimum index considered in training for target var\n",
    "    max_idx - maximum index considered in training for target var\n",
    "    '''\n",
    "    #now = time()\n",
    "    prediction = model.predict(predict_x)\n",
    "    \n",
    "    # Convert data to array of pdfs indexed by training example\n",
    "    predict_pdfs = np.zeros((len(predict_x), n_classes))\n",
    "\n",
    "    predict_pdfs[:, min_idx:max_idx+1] = prediction\n",
    "    \n",
    "    # can't predict probability of gaining more yards than end zone,\n",
    "    # so instead: drop and re-normalize?\n",
    "    max_target_cls_idx = yardline100 + 99\n",
    "    for idx, predict_row in enumerate(predict_pdfs):\n",
    "        max_idx = max_target_cls_idx[idx]\n",
    "        #predict_pdfs[idx, max_idx] = np.sum(predict_row[max_idx:])\n",
    "        predict_pdfs[idx, max_idx+1:] = 0.0\n",
    "        # Now renormalize:\n",
    "        predict_pdfs[idx, :] = predict_pdfs[idx, :]/predict_pdfs[idx, :].sum()\n",
    "    \n",
    "    # convert to cdfs:\n",
    "    predict_cdfs = np.cumsum(predict_pdfs, axis=1)\n",
    "    return predict_cdfs, predict_pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_play_and_player_cols(df,predicting=False):\n",
    "    df['IsRusher'] = df['NflId'] == df['NflIdRusher']\n",
    "    \n",
    "    play_ids = df[\"PlayId\"].unique()\n",
    "    \n",
    "    # We must assume here that the first 22 rows correspond to the same player:\n",
    "    player_cols = [\n",
    "        'PlayId', # This is the link between them\n",
    "        'Season',\n",
    "        'Team',\n",
    "        'X',\n",
    "        'Y',\n",
    "        'S',\n",
    "        'A',\n",
    "        'Dis',\n",
    "        'Dir',\n",
    "        'NflId',\n",
    "        'IsRusher',\n",
    "    ]\n",
    "\n",
    "    df_players = df[player_cols]\n",
    "    \n",
    "    play_cols = [\n",
    "        'PlayId',\n",
    "        'Season',\n",
    "        'PossessionTeam',\n",
    "        'HomeTeamAbbr',\n",
    "        'VisitorTeamAbbr',\n",
    "        'PlayDirection', \n",
    "        'FieldPosition',\n",
    "        'YardLine',\n",
    "    ]\n",
    "    if not predicting:\n",
    "        play_cols.append('Yards')\n",
    "        \n",
    "    df_play = df[play_cols].copy()\n",
    "\n",
    "    ## Fillna in FieldPosition attribute\n",
    "    #df['FieldPosition'] = df.groupby(['PlayId'], sort=False)['FieldPosition'].apply(lambda x: x.ffill().bfill())\n",
    "    \n",
    "    # Get first \n",
    "    df_play = df_play.groupby('PlayId').first().reset_index()\n",
    "\n",
    "    #print('rows/plays in df: ', len(df_play))\n",
    "    assert df_play.PlayId.nunique() == df.PlayId.nunique(), \"Play/player split failed?\"  # Boom\n",
    "    \n",
    "    return df_play, df_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_one_play(df_test):\n",
    "    df_play, df_players_split = split_play_and_player_cols(df_test, predicting=True)\n",
    "    \n",
    "    process_team_abbr(df_play)\n",
    "    process_play_direction(df_play)\n",
    "    process_yard_til_end_zone(df_play)\n",
    "    \n",
    "    df_players = df_players_split.merge(\n",
    "        df_play[['PlayId','PossessionTeam','HomeTeamAbbr','PlayDirection', 'Yardline100']], \n",
    "        how='left', on='PlayId')\n",
    "    \n",
    "    standarize_direction(df_players)\n",
    "    process_tracking_data(df_players)\n",
    "    df_all_feats = df_players[tracking_level_features]\n",
    "\n",
    "  \n",
    "    train_x = np.zeros([1,11,10,10])\n",
    "    [[rusher_x, rusher_y, rusher_Sx, rusher_Sy]] = df_all_feats.loc[df_all_feats.IsRusher==1,['X_std', 'Y_std','Sx','Sy']].values\n",
    "\n",
    "    offense_ids = df_all_feats[df_all_feats.IsOnOffense & ~df_all_feats.IsRusher].index\n",
    "    defense_ids = df_all_feats[~df_all_feats.IsOnOffense].index\n",
    "\n",
    "    for j, defense_id in enumerate(defense_ids):\n",
    "        [def_x, def_y, def_Sx, def_Sy] = df_all_feats.loc[defense_id,['X_std', 'Y_std','Sx','Sy']].values\n",
    "        [def_rusher_x, def_rusher_y] = df_all_feats.loc[defense_id,['player_minus_rusher_x', 'player_minus_rusher_y']].values\n",
    "        [def_rusher_Sx, def_rusher_Sy] =  df_all_feats.loc[defense_id,['player_minus_rusher_Sx', 'player_minus_rusher_Sy']].values\n",
    "\n",
    "        train_x[0,j,:,:4] = df_all_feats.loc[offense_ids,['Sx','Sy','X_std', 'Y_std']].values - np.array([def_Sx, def_Sy, def_x,def_y])\n",
    "        train_x[0,j,:,-6:] = [def_rusher_Sx, def_rusher_Sy, def_rusher_x, def_rusher_y, def_Sx, def_Sy]\n",
    "\n",
    "\n",
    "    yardline100 = df_play['Yardline100'].values\n",
    "    return [train_x, yardline100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test play 0...\n",
      "Processing test play 10...\n",
      "Processing test play 20...\n",
      "Processing test play 30...\n",
      "Processing test play 40...\n",
      "Processing test play 50...\n",
      "Processing test play 60...\n",
      "Processing test play 70...\n",
      "Processing test play 80...\n",
      "Processing test play 90...\n",
      "Processing test play 100...\n",
      "Processing test play 110...\n",
      "Processing test play 120...\n",
      "Processing test play 130...\n",
      "Processing test play 140...\n",
      "Processing test play 150...\n",
      "Processing test play 160...\n",
      "Processing test play 170...\n",
      "Processing test play 180...\n",
      "Processing test play 190...\n",
      "Processing test play 200...\n",
      "Processing test play 210...\n",
      "Processing test play 220...\n",
      "Processing test play 230...\n",
      "Processing test play 240...\n",
      "Processing test play 250...\n",
      "Processing test play 260...\n",
      "Processing test play 270...\n",
      "Processing test play 280...\n",
      "Processing test play 290...\n",
      "Processing test play 300...\n",
      "Processing test play 310...\n",
      "Processing test play 320...\n",
      "Processing test play 330...\n",
      "Processing test play 340...\n",
      "Processing test play 350...\n",
      "Processing test play 360...\n",
      "Processing test play 370...\n",
      "Processing test play 380...\n",
      "Processing test play 390...\n",
      "Processing test play 400...\n",
      "Processing test play 410...\n",
      "Processing test play 420...\n",
      "Processing test play 430...\n",
      "Processing test play 440...\n",
      "Processing test play 450...\n",
      "Processing test play 460...\n",
      "Processing test play 470...\n",
      "Processing test play 480...\n",
      "Processing test play 490...\n",
      "Processing test play 500...\n",
      "Processing test play 510...\n",
      "Processing test play 520...\n",
      "Processing test play 530...\n",
      "Processing test play 540...\n",
      "Processing test play 550...\n",
      "Processing test play 560...\n",
      "Processing test play 570...\n",
      "Processing test play 580...\n",
      "Processing test play 590...\n",
      "Processing test play 600...\n",
      "Processing test play 610...\n",
      "Processing test play 620...\n",
      "Processing test play 630...\n",
      "Processing test play 640...\n",
      "Processing test play 650...\n",
      "Processing test play 660...\n",
      "Processing test play 670...\n",
      "Processing test play 680...\n",
      "Processing test play 690...\n",
      "Processing test play 700...\n",
      "Processing test play 710...\n",
      "Processing test play 720...\n",
      "Processing test play 730...\n",
      "Processing test play 740...\n",
      "Processing test play 750...\n",
      "Processing test play 760...\n",
      "Processing test play 770...\n",
      "Processing test play 780...\n",
      "Processing test play 790...\n",
      "Processing test play 800...\n",
      "Processing test play 810...\n",
      "Processing test play 820...\n",
      "Processing test play 830...\n",
      "Processing test play 840...\n",
      "Processing test play 850...\n",
      "Processing test play 860...\n",
      "Processing test play 870...\n",
      "Processing test play 880...\n",
      "Processing test play 890...\n",
      "Processing test play 900...\n",
      "Processing test play 910...\n",
      "Processing test play 920...\n",
      "Processing test play 930...\n",
      "Processing test play 940...\n",
      "Processing test play 950...\n",
      "Processing test play 960...\n",
      "Processing test play 970...\n",
      "Processing test play 980...\n",
      "Processing test play 990...\n",
      "Processing test play 1000...\n",
      "Processing test play 1010...\n",
      "Processing test play 1020...\n",
      "Processing test play 1030...\n",
      "Processing test play 1040...\n",
      "Processing test play 1050...\n",
      "Processing test play 1060...\n",
      "Processing test play 1070...\n",
      "Processing test play 1080...\n",
      "Processing test play 1090...\n",
      "Processing test play 1100...\n",
      "Processing test play 1110...\n",
      "Processing test play 1120...\n",
      "Processing test play 1130...\n",
      "Processing test play 1140...\n",
      "Processing test play 1150...\n",
      "Processing test play 1160...\n",
      "Processing test play 1170...\n",
      "Processing test play 1180...\n",
      "Processing test play 1190...\n",
      "Processing test play 1200...\n",
      "Processing test play 1210...\n",
      "Processing test play 1220...\n",
      "Processing test play 1230...\n",
      "Processing test play 1240...\n",
      "Processing test play 1250...\n",
      "Processing test play 1260...\n",
      "Processing test play 1270...\n",
      "Processing test play 1280...\n",
      "Processing test play 1290...\n",
      "Processing test play 1300...\n",
      "Processing test play 1310...\n",
      "Processing test play 1320...\n",
      "Processing test play 1330...\n",
      "Processing test play 1340...\n",
      "Processing test play 1350...\n",
      "Processing test play 1360...\n",
      "Processing test play 1370...\n",
      "Processing test play 1380...\n",
      "Processing test play 1390...\n",
      "Processing test play 1400...\n",
      "Processing test play 1410...\n",
      "Processing test play 1420...\n",
      "Processing test play 1430...\n",
      "Processing test play 1440...\n",
      "Processing test play 1450...\n",
      "Processing test play 1460...\n",
      "Processing test play 1470...\n",
      "Processing test play 1480...\n",
      "Processing test play 1490...\n",
      "Processing test play 1500...\n",
      "Processing test play 1510...\n",
      "Processing test play 1520...\n",
      "Processing test play 1530...\n",
      "Processing test play 1540...\n",
      "Processing test play 1550...\n",
      "Processing test play 1560...\n",
      "Processing test play 1570...\n",
      "Processing test play 1580...\n",
      "Processing test play 1590...\n",
      "Processing test play 1600...\n",
      "Processing test play 1610...\n",
      "Processing test play 1620...\n",
      "Processing test play 1630...\n",
      "Processing test play 1640...\n",
      "Processing test play 1650...\n",
      "Processing test play 1660...\n",
      "Processing test play 1670...\n",
      "Processing test play 1680...\n",
      "Processing test play 1690...\n",
      "Processing test play 1700...\n",
      "Processing test play 1710...\n",
      "Processing test play 1720...\n",
      "Processing test play 1730...\n",
      "Processing test play 1740...\n",
      "Processing test play 1750...\n",
      "Processing test play 1760...\n",
      "Processing test play 1770...\n",
      "Processing test play 1780...\n",
      "Processing test play 1790...\n",
      "Processing test play 1800...\n",
      "Processing test play 1810...\n",
      "Processing test play 1820...\n",
      "Processing test play 1830...\n",
      "Processing test play 1840...\n",
      "Processing test play 1850...\n",
      "Processing test play 1860...\n",
      "Processing test play 1870...\n",
      "Processing test play 1880...\n",
      "Processing test play 1890...\n",
      "Processing test play 1900...\n",
      "Processing test play 1910...\n",
      "Processing test play 1920...\n",
      "Processing test play 1930...\n",
      "Processing test play 1940...\n",
      "Processing test play 1950...\n",
      "Processing test play 1960...\n",
      "Processing test play 1970...\n",
      "Processing test play 1980...\n",
      "Processing test play 1990...\n",
      "Processing test play 2000...\n",
      "Processing test play 2010...\n",
      "Processing test play 2020...\n",
      "Processing test play 2030...\n",
      "Processing test play 2040...\n",
      "Processing test play 2050...\n",
      "Processing test play 2060...\n",
      "Processing test play 2070...\n",
      "Processing test play 2080...\n",
      "Processing test play 2090...\n",
      "Processing test play 2100...\n",
      "Processing test play 2110...\n",
      "Processing test play 2120...\n",
      "Processing test play 2130...\n",
      "Processing test play 2140...\n",
      "Processing test play 2150...\n",
      "Processing test play 2160...\n",
      "Processing test play 2170...\n",
      "Processing test play 2180...\n",
      "Processing test play 2190...\n",
      "Processing test play 2200...\n",
      "Processing test play 2210...\n",
      "Processing test play 2220...\n",
      "Processing test play 2230...\n",
      "Processing test play 2240...\n",
      "Processing test play 2250...\n",
      "Processing test play 2260...\n",
      "Processing test play 2270...\n",
      "Processing test play 2280...\n",
      "Processing test play 2290...\n",
      "Processing test play 2300...\n",
      "Processing test play 2310...\n",
      "Processing test play 2320...\n",
      "Processing test play 2330...\n",
      "Processing test play 2340...\n",
      "Processing test play 2350...\n",
      "Processing test play 2360...\n",
      "Processing test play 2370...\n",
      "Processing test play 2380...\n",
      "Processing test play 2390...\n",
      "Processing test play 2400...\n",
      "Processing test play 2410...\n",
      "Processing test play 2420...\n",
      "Processing test play 2430...\n",
      "Processing test play 2440...\n",
      "Processing test play 2450...\n",
      "Processing test play 2460...\n",
      "Processing test play 2470...\n",
      "Processing test play 2480...\n",
      "Processing test play 2490...\n",
      "Processing test play 2500...\n",
      "Processing test play 2510...\n",
      "Processing test play 2520...\n",
      "Processing test play 2530...\n",
      "Processing test play 2540...\n",
      "Processing test play 2550...\n",
      "Processing test play 2560...\n",
      "Processing test play 2570...\n",
      "Processing test play 2580...\n",
      "Processing test play 2590...\n",
      "Processing test play 2600...\n",
      "Processing test play 2610...\n",
      "Processing test play 2620...\n",
      "Processing test play 2630...\n",
      "Processing test play 2640...\n",
      "Processing test play 2650...\n",
      "Processing test play 2660...\n",
      "Processing test play 2670...\n",
      "Processing test play 2680...\n",
      "Processing test play 2690...\n",
      "Processing test play 2700...\n",
      "Processing test play 2710...\n",
      "Processing test play 2720...\n",
      "Processing test play 2730...\n",
      "Processing test play 2740...\n",
      "Processing test play 2750...\n",
      "Processing test play 2760...\n",
      "Processing test play 2770...\n",
      "Processing test play 2780...\n",
      "Processing test play 2790...\n",
      "Processing test play 2800...\n",
      "Processing test play 2810...\n",
      "Processing test play 2820...\n",
      "Processing test play 2830...\n",
      "Processing test play 2840...\n",
      "Processing test play 2850...\n",
      "Processing test play 2860...\n",
      "Processing test play 2870...\n",
      "Processing test play 2880...\n",
      "Processing test play 2890...\n",
      "Processing test play 2900...\n",
      "Processing test play 2910...\n",
      "Processing test play 2920...\n",
      "Processing test play 2930...\n",
      "Processing test play 2940...\n",
      "Processing test play 2950...\n",
      "Processing test play 2960...\n",
      "Processing test play 2970...\n",
      "Processing test play 2980...\n",
      "Processing test play 2990...\n",
      "Processing test play 3000...\n",
      "Processing test play 3010...\n",
      "Processing test play 3020...\n",
      "Processing test play 3030...\n",
      "Processing test play 3040...\n",
      "Processing test play 3050...\n",
      "Processing test play 3060...\n",
      "Processing test play 3070...\n",
      "Processing test play 3080...\n",
      "Processing test play 3090...\n",
      "Processing test play 3100...\n",
      "Processing test play 3110...\n",
      "Processing test play 3120...\n",
      "Processing test play 3130...\n",
      "Processing test play 3140...\n",
      "Processing test play 3150...\n",
      "Processing test play 3160...\n",
      "Processing test play 3170...\n",
      "Processing test play 3180...\n",
      "Processing test play 3190...\n",
      "Processing test play 3200...\n",
      "Processing test play 3210...\n",
      "Processing test play 3220...\n",
      "Processing test play 3230...\n",
      "Processing test play 3240...\n",
      "Processing test play 3250...\n",
      "Processing test play 3260...\n",
      "Processing test play 3270...\n",
      "Processing test play 3280...\n",
      "Processing test play 3290...\n",
      "Processing test play 3300...\n",
      "Processing test play 3310...\n",
      "Processing test play 3320...\n",
      "Processing test play 3330...\n",
      "Processing test play 3340...\n",
      "Processing test play 3350...\n",
      "Processing test play 3360...\n",
      "Processing test play 3370...\n",
      "Processing test play 3380...\n",
      "Processing test play 3390...\n",
      "Processing test play 3400...\n",
      "Processing test play 3410...\n",
      "Processing test play 3420...\n",
      "Processing test play 3430...\n",
      "Processing test play 3440...\n",
      "Processing test play 3450...\n",
      "Processing test play 3460...\n",
      "Processing test play 3470...\n",
      "Processing test play 3480...\n",
      "Processing test play 3490...\n",
      "Processing test play 3500...\n",
      "Processing test play 3510...\n",
      "Processing test play 3520...\n",
      "Processing test play 3530...\n",
      "Processing test play 3540...\n",
      "Processing test play 3550...\n",
      "Processing test play 3560...\n",
      "Processing test play 3570...\n",
      "Processing test play 3580...\n",
      "Processing test play 3590...\n",
      "Processing test play 3600...\n",
      "Processing test play 3610...\n",
      "Processing test play 3620...\n"
     ]
    }
   ],
   "source": [
    "iter_test = env.iter_test()\n",
    "for iplay, (test_df, sample_prediction_df) in enumerate(iter_test):\n",
    "    if iplay % 10 == 0:\n",
    "        print('Processing test play {}...'.format(iplay))\n",
    "    [inference_row, yardline100] = extract_features_one_play(test_df)\n",
    "    \n",
    "    cdfs_arr = []\n",
    "    for model in models:\n",
    "        cdfs_, pdfs_ = get_cdf_prediction_model(\n",
    "            inference_row, \n",
    "            n_classes=199, \n",
    "            model=model, \n",
    "            min_idx=min_idx_y, \n",
    "            max_idx=max_idx_y,\n",
    "            yardline100=yardline100)\n",
    "        cdfs_arr.append(cdfs_)\n",
    "    cdfs = np.mean(cdfs_arr, axis=0)\n",
    "\n",
    "    # To avoid rounding error:\n",
    "    cdf_val = cdfs[0].clip(0.0,1.0)\n",
    "    \n",
    "    for icol, col in enumerate(sample_prediction_df.columns):\n",
    "        sample_prediction_df.loc[:, col] = cdf_val[icol]\n",
    "    env.predict(sample_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission file has been saved!  Once you `Commit` your Notebook and it finishes running, you can submit the file to the competition from the Notebook Viewer `Output` tab.\n",
      "['submission.csv']\n"
     ]
    }
   ],
   "source": [
    "env.write_submission_file()\n",
    "print([filename for filename in os.listdir('/kaggle/working') if '.csv' in filename])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
